{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best-Feature Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyltr in /opt/conda/lib/python3.7/site-packages (0.2.4)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from pyltr) (1.0.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from pyltr) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from pyltr) (0.22.2.post1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from pyltr) (1.18.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->pyltr) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas->pyltr) (2.8.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->pyltr) (0.14.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas->pyltr) (1.14.0)\n",
      "Requirement already satisfied: more_itertools in /opt/conda/lib/python3.7/site-packages (8.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyltr\n",
    "!pip install more_itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pyltr\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from more_itertools import sort_together\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.metrics import label_ranking_average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input array of qids of each query-document pair\n",
    "# output sorted qids \n",
    "def get_qids(Qids):\n",
    "    qs = list(set(Qids))\n",
    "    qs.sort()\n",
    "    \n",
    "    return qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_MRR(relevance_list, feature_list,):\n",
    "    # sort arrays by the feature value\n",
    "    feature_list, relevance_list = (list(t) for t in zip(*sorted(zip(feature_list, relevance_list), reverse = True)))\n",
    "    # MRR@100, only look at top 100\n",
    "    if len(feature_list) > 100:\n",
    "        feature_list = feature_list[0:100]\n",
    "        relevance_list = relevance_list[0:100]\n",
    "\n",
    "    idx = np.where(relevance_list)[0]#get the indexesof relevant document\n",
    "    if any(idx):\n",
    "        return 1/(idx[0]+1) # index starts from 0 so add 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input training data\n",
    "## output the index of best feature according to ndcg@5\n",
    "def find_best_feature(TX, Ty, Tqids):\n",
    "    # get a list of qids\n",
    "    tqs = get_qids(Tqids)\n",
    "    \n",
    "    ndcg_features = np.empty((0,np.size(tqs)), float) # store ndcg@5 of each qid for each feature\n",
    "\n",
    "    # iterate over features\n",
    "    for feature_no in range(np.size(TX, 1)):\n",
    "        single_feature = [np.array(i[feature_no]) for i in TX]\n",
    "        ndcg_qs = np.array([])\n",
    "        # iterate over qid\n",
    "        for i in tqs: \n",
    "            idxs = np.where(Tqids == i)[0]\n",
    "            feature_list = [single_feature[idx] for idx in idxs] # target score\n",
    "            relevance_list = [Ty[idx] for idx in idxs] # true score\n",
    "            # get the evaluation score for the query\n",
    "            score = ndcg_score(np.asarray([relevance_list]), np.asarray([feature_list]), k=5)\n",
    "            ndcg_qs = np.append(ndcg_qs, [score], axis = 0)\n",
    "\n",
    "        ndcg_features = np.append(ndcg_features, [ndcg_qs], axis = 0)\n",
    "            \n",
    "    # get the average ndcg score for each feature\n",
    "    feature_avg_score = np.mean(ndcg_features, axis=1)\n",
    "    \n",
    "    # return index of the feature with highest score\n",
    "    return np.argmax(feature_avg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input evaluation data and index of best feature from training \n",
    "## output an array of ndcg score for each qids\n",
    "def evaluate_BFC_NDCG(EX, Ey, Eqids, best_feature):\n",
    "    \n",
    "    # get a list of qids\n",
    "    eqs = get_qids(Eqids)\n",
    "    \n",
    "    EX_best_feature = [np.array(i[best_feature]) for i in EX]\n",
    "    ndcg_qs = np.array([])\n",
    "    # iterate over qid\n",
    "    for i in eqs: \n",
    "        idxs = np.where(Eqids == i)[0]\n",
    "        feature_list = [EX_best_feature[idx] for idx in idxs] # target score\n",
    "        relevance_list = [Ey[idx] for idx in idxs] # true score\n",
    "        # get the evaluation score for the query\n",
    "        score = ndcg_score(np.asarray([relevance_list]), np.asarray([feature_list]), k=5)\n",
    "        ndcg_qs = np.append(ndcg_qs, [score], axis = 0)\n",
    "\n",
    "    return ndcg_qs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input evaluation data and index of best feature from training \n",
    "## output an array of MRR score for each qids\n",
    "def evaluate_BFC_MRR(EX, Ey, Eqids, best_feature):\n",
    "    \n",
    "    # get a list of qids\n",
    "    eqs = get_qids(Eqids)\n",
    "    \n",
    "    EX_best_feature = [np.array(i[best_feature]) for i in EX]\n",
    "    mrr_qs = np.array([])\n",
    "    # iterate over qid\n",
    "    for i in eqs: \n",
    "        idxs = np.where(Eqids == i)[0]\n",
    "        feature_list = [EX_best_feature[idx] for idx in idxs] # target score\n",
    "        relevance_list = [Ey[idx] > 0 for idx in idxs] # true score, convert a boolean list (relevant or non relevant)\n",
    "        # get the evaluation score for the query\n",
    "        score = compute_MRR(relevance_list, feature_list)\n",
    "        mrr_qs = np.append(mrr_qs, [score], axis = 0)\n",
    "\n",
    "    return mrr_qs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "direc_name = './MQ2007/Fold'\n",
    "train_name = '/train.txt'\n",
    "valid_name = '/vali.txt'\n",
    "test_name = '/test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = np.array([]) # store best features for each fold\n",
    "qids = np.array([]) # store all the qids\n",
    "ndcg_qs = np.array([]) # store all ndcg scores\n",
    "mrr_qs = np.array([]) # store all mrr scores\n",
    "\n",
    "for fold_no in range(5):\n",
    "    with open(direc_name + str(fold_no+1) + train_name) as trainfile, \\\n",
    "             open(direc_name + str(fold_no+1) + valid_name) as valifile, \\\n",
    "             open(direc_name + str(fold_no+1) + test_name) as evalfile:\n",
    "        TX, Ty, Tqids, _ = pyltr.data.letor.read_dataset(trainfile)\n",
    "        VX, Vy, Vqids, _ = pyltr.data.letor.read_dataset(valifile)\n",
    "        EX, Ey, Eqids, _ = pyltr.data.letor.read_dataset(evalfile)\n",
    "    \n",
    "    Tqids = np.array([int(i) for i in Tqids])\n",
    "    Vqids = np.array([int(i) for i in Vqids])\n",
    "    Eqids = np.array([int(i) for i in Eqids])\n",
    "    \n",
    "    e_qids = np.array(get_qids(Eqids))\n",
    "    \n",
    "    best_feature = find_best_feature(TX, Ty, Tqids)\n",
    "    eval_ndcg = evaluate_BFC_NDCG(EX, Ey, Eqids, best_feature)\n",
    "    eval_mrr = evaluate_BFC_MRR(EX, Ey, Eqids, best_feature)\n",
    "    \n",
    "    best_features = np.append(best_features, [best_feature])\n",
    "    qids = np.append(qids, e_qids)\n",
    "    ndcg_qs = np.append(ndcg_qs, eval_ndcg)\n",
    "    mrr_qs = np.append(mrr_qs, eval_mrr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([38.])"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4297203061551881"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ndcg_qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5502663262322336"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(mrr_qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
