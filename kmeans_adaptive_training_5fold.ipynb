{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyltr in /opt/conda/lib/python3.7/site-packages (0.2.4)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from pyltr) (1.4.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from pyltr) (1.0.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from pyltr) (1.18.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from pyltr) (0.22.2.post1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->pyltr) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas->pyltr) (2.8.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->pyltr) (0.14.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas->pyltr) (1.14.0)\n",
      "Requirement already satisfied: more_itertools in /opt/conda/lib/python3.7/site-packages (8.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyltr\n",
    "!pip install more_itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import pyltr\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "from more_itertools import sort_together\n",
    "import collections\n",
    "import heapq\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input array of qids of each query-document pair\n",
    "# output sorted qids \n",
    "def get_qids(Qids):\n",
    "    qs = list(set(Qids))\n",
    "    qs.sort()\n",
    "    \n",
    "    return qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input evaluation data and cluster number\n",
    "## output an array of ndcg score for each qids\n",
    "def evaluate_BFC_NDCG_selective_cluster(EX, Ey, Eqids, E_k, models):\n",
    "    \n",
    "    # get a list of qids\n",
    "    eqs = get_qids(Eqids)\n",
    "    metric = pyltr.metrics.NDCG(k=5)\n",
    "    ndcg_qs = np.array([])\n",
    "    \n",
    "    for i, eq in enumerate(eqs):\n",
    "        idxs = np.where(Eqids == eq)[0]\n",
    "        EX_i = [EX[idx] for idx in idxs] # target score\n",
    "        relevance_list = [Ey[idx] for idx in idxs] # true score\n",
    "        \n",
    "        # select model from predicted cluster\n",
    "        model = models[E_k[i]]\n",
    "        Epred_i = model.predict(EX_i)\n",
    "        \n",
    "        score = metric.evaluate_preds(i, np.asarray(relevance_list), np.asarray(Epred_i))\n",
    "        ndcg_qs = np.append(ndcg_qs, [score], axis = 0)\n",
    "        \n",
    "    return ndcg_qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input evaluation data and cluster number\n",
    "## output an array of ndcg score for each qids\n",
    "def evaluate_BFC_NDCG_cluster_oracle(EX, Ey, Eqids, no_cluster, models):\n",
    "    \n",
    "    # get a list of qids\n",
    "    eqs = get_qids(Eqids)\n",
    "    metric = pyltr.metrics.NDCG(k=5)\n",
    "    ndcg_qs = np.array([])\n",
    "    \n",
    "    for i, eq in enumerate(eqs):\n",
    "        idxs = np.where(Eqids == eq)[0]\n",
    "        EX_i = [EX[idx] for idx in idxs] # target score\n",
    "        relevance_list = [Ey[idx] for idx in idxs] # true score\n",
    "        \n",
    "        best = 0\n",
    "        # choose highest performance of 5 models\n",
    "        for k in range(no_cluster):\n",
    "            model = models[k]\n",
    "            Epred_i = model.predict(EX_i)\n",
    "            score = metric.evaluate_preds(i, np.asarray(relevance_list), np.asarray(Epred_i))\n",
    "            if score > best:\n",
    "                best = score\n",
    "        \n",
    "\n",
    "        ndcg_qs = np.append(ndcg_qs, [best], axis = 0)\n",
    "        \n",
    "    return ndcg_qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input evaluation data and cluster number\n",
    "## output an array of ndcg score for each qids\n",
    "def evaluate_BFC_NDCG_cluster_fusion(EX, Ey, Eqids, prob_dist, no_cluster,  models):\n",
    "    \n",
    "    # get a list of qids\n",
    "    eqs = get_qids(Eqids)\n",
    "    metric = pyltr.metrics.NDCG(k=5)\n",
    "    ndcg_qs = np.array([])\n",
    "    \n",
    "    for i, eq in enumerate(eqs):\n",
    "        idxs = np.where(Eqids == eq)[0]\n",
    "        EX_i = [EX[idx] for idx in idxs] # target score\n",
    "        relevance_list = [Ey[idx] for idx in idxs] # true score\n",
    "        \n",
    "        Epred_i = np.zeros(len(idxs))\n",
    "        for k in range(no_cluster):\n",
    "            model = models[k]\n",
    "            pred = model.predict(EX_i) * prob_dist[i][k]\n",
    "            Epred_i = Epred_i + pred\n",
    "        \n",
    "        score = metric.evaluate_preds(i, np.asarray(relevance_list), np.asarray(Epred_i))\n",
    "        \n",
    "\n",
    "        ndcg_qs = np.append(ndcg_qs, [score], axis = 0)\n",
    "        \n",
    "    return ndcg_qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input evaluation data and cluster number\n",
    "## output an array of MRR score for each qids\n",
    "def evaluate_BFC_MRR_selective_cluster(EX, Ey, Eqids, E_k, models):\n",
    "    \n",
    "    # get a list of qids\n",
    "    eqs = get_qids(Eqids)\n",
    "    mrr_qs = np.array([])\n",
    "    \n",
    "    \n",
    "    for i, eq in enumerate(eqs):\n",
    "        idxs = np.where(Eqids == eq)[0]\n",
    "        EX_i = [EX[idx] for idx in idxs] # target score\n",
    "        relevance_list = [Ey[idx] > 0 for idx in idxs] # true score, convert a boolean list (relevant or non relevant)\n",
    "        \n",
    "        # select model from predicted cluster\n",
    "        model = models[E_k[i]]\n",
    "        Epred_i = model.predict(EX_i)\n",
    "        \n",
    "        # get the evaluation score for the query\n",
    "        score = compute_MRR(relevance_list, np.asarray(Epred_i))\n",
    "        mrr_qs = np.append(mrr_qs, [score], axis = 0)\n",
    "\n",
    "    return mrr_qs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input evaluation data and cluster number\n",
    "## output an array of MRR score for each qids\n",
    "def evaluate_BFC_MRR_cluster_oracle(EX, Ey, Eqids, no_cluster, models):\n",
    "    \n",
    "    # get a list of qids\n",
    "    eqs = get_qids(Eqids)\n",
    "    mrr_qs = np.array([])\n",
    "    \n",
    "    \n",
    "    for i, eq in enumerate(eqs):\n",
    "        idxs = np.where(Eqids == eq)[0]\n",
    "        EX_i = [EX[idx] for idx in idxs] # target score\n",
    "        relevance_list = [Ey[idx] > 0 for idx in idxs] # true score, convert a boolean list (relevant or non relevant)\n",
    "        \n",
    "        best = 0\n",
    "        # choose highest performance of 5 models\n",
    "        for k in range(no_cluster):\n",
    "            model = models[k]\n",
    "            Epred_i = model.predict(EX_i)\n",
    "            score = compute_MRR(relevance_list, np.asarray(Epred_i))\n",
    "            if score > best:\n",
    "                best = score\n",
    "\n",
    "        mrr_qs = np.append(mrr_qs, [best], axis = 0)\n",
    "\n",
    "    return mrr_qs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input evaluation data and probability distributions of clusters\n",
    "## output an array of MRR score for each qids\n",
    "def evaluate_BFC_MRR_cluster_fusion(EX, Ey, Eqids, prob_dist, no_cluster, models):\n",
    "    # get a list of qids\n",
    "    eqs = get_qids(Eqids)\n",
    "    mrr_qs = np.array([])\n",
    "    \n",
    "    \n",
    "    for i, eq in enumerate(eqs):\n",
    "        idxs = np.where(Eqids == eq)[0]\n",
    "        EX_i = [EX[idx] for idx in idxs] # target score\n",
    "        relevance_list = [Ey[idx] > 0 for idx in idxs] # true score, convert a boolean list (relevant or non relevant)\n",
    "        \n",
    "        \n",
    "        Epred_i = np.zeros(len(idxs))\n",
    "        for k in range(no_cluster):\n",
    "            model = models[k]\n",
    "            pred = model.predict(EX_i) * prob_dist[i][k]\n",
    "            Epred_i = Epred_i + pred\n",
    "            \n",
    "        score = compute_MRR(relevance_list, np.asarray(Epred_i))\n",
    "        mrr_qs = np.append(mrr_qs, [score], axis = 0)\n",
    "\n",
    "    return mrr_qs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision(relevance_list, feature_list, k):\n",
    "    feature_list, relevance_list = (list(t) for t in zip(*sorted(zip(feature_list, relevance_list), reverse = True)))\n",
    "    if len(feature_list) > k:\n",
    "        feature_list = feature_list[0:k]\n",
    "        relevance_list = relevance_list[0:k]\n",
    "    else:\n",
    "        k = len(feature_list)\n",
    "    \n",
    "    return np.count_nonzero(relevance_list)/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_MRR(relevance_list, feature_list):\n",
    "    # sort arrays by the feature value\n",
    "    feature_list, relevance_list = (list(t) for t in zip(*sorted(zip(feature_list, relevance_list), reverse = True)))\n",
    "    # MRR@100, only look at top 100\n",
    "    if len(feature_list) > 100:\n",
    "        feature_list = feature_list[0:100]\n",
    "        relevance_list = relevance_list[0:100]\n",
    "\n",
    "    idx = np.where(relevance_list)[0]#get the indexesof relevant document\n",
    "    if any(idx):\n",
    "        return 1/(idx[0]+1) # index starts from 0 so add 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "direc_name = './MQ2007/Fold'\n",
    "train_name = '/train.txt'\n",
    "valid_name = '/vali.txt'\n",
    "test_name = '/test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold number: 0\n",
      "loop count: 0\n",
      "loop count: 1\n",
      "loop count: 2\n",
      "fold number: 1\n",
      "loop count: 0\n",
      "loop count: 1\n",
      "loop count: 2\n",
      "fold number: 2\n",
      "loop count: 0\n",
      "loop count: 1\n",
      "loop count: 2\n",
      "fold number: 3\n",
      "loop count: 0\n",
      "loop count: 1\n",
      "loop count: 2\n",
      "fold number: 4\n",
      "loop count: 0\n",
      "loop count: 1\n",
      "loop count: 2\n"
     ]
    }
   ],
   "source": [
    "Qid = np.array([]) # store all the qids\n",
    "e_ndcg_sc = np.array([]) # store all ndcg scores of selective cluster\n",
    "e_mrr_sc = np.array([]) # store all mrr scores of selective cluster\n",
    "e_ndcg_co = np.array([]) # store all ndcg scores of cluster oracle\n",
    "e_mrr_co = np.array([]) # store all mrr scores of cluster oracle\n",
    "e_ndcg_cf = np.array([]) # store all ndcg scores of cluster fusion\n",
    "e_mrr_cf = np.array([]) # store all mrr scores of cluster fusion\n",
    "\n",
    "for fold_no in range(5):\n",
    "    print('fold number: ' + str(fold_no))\n",
    "    with open(direc_name + str(fold_no+1) + train_name) as trainfile, \\\n",
    "             open(direc_name + str(fold_no+1) + test_name) as evalfile:\n",
    "        TX, Ty, Tqids, _ = pyltr.data.letor.read_dataset(trainfile)\n",
    "        EX, Ey, Eqids, _ = pyltr.data.letor.read_dataset(evalfile)\n",
    "    \n",
    "    Tqids = np.array([int(i) for i in Tqids])\n",
    "    Eqids = np.array([int(i) for i in Eqids])\n",
    "    \n",
    "    Qids_e = np.array(get_qids(Eqids))\n",
    "    \n",
    "    # get the qids in the trainind data\n",
    "    qids = get_qids(Tqids)\n",
    "    \n",
    "    ### Average of features of relevant documents for query IDs\n",
    "    \n",
    "    avg_features_qids = np.empty((0,np.size(TX, 1)), float)\n",
    "\n",
    "    # iterate over qid\n",
    "    for i in qids: \n",
    "        idxs = np.where(Tqids == i)[0] # retrieve indexes of corresponding qid\n",
    "        relevance_list = [Ty[idx] > 0 for idx in idxs] # list of boolean(relevant) of q-d pairs\n",
    "        rel_idxs =np.where(relevance_list)[0] # retrieve indexes of relevant docs\n",
    "    \n",
    "        ### if np where is never empty\n",
    "        rel_doc_list = [TX[idx] for idx in rel_idxs] # retrieve relevant docs\n",
    "        avg_features = np.mean(rel_doc_list, axis=0) #average each feature of relevant docs\n",
    "    \n",
    "        if any(rel_idxs):\n",
    "            rel_doc_list = [TX[idx] for idx in rel_idxs]\n",
    "            avg_features = np.mean(rel_doc_list, axis=0) #average each feature of relevant docs\n",
    "        else:\n",
    "            avg_features = np.zeros(np.size(TX, 1)) # if there is no relevant document in training, give features 0s\n",
    "    \n",
    "        avg_features_qids = np.append(avg_features_qids, [avg_features], axis = 0)\n",
    "\n",
    "    q_vec = avg_features_qids\n",
    "    \n",
    "    \n",
    "    ###  Cluster once with average features of relevant docs, and twice more using evaluation metrics\n",
    "    \n",
    "    NO_OF_CLUSTER=5\n",
    "\n",
    "    for count in range(3):\n",
    "        print('loop count: ' + str(count))\n",
    "        ##cluster the feature into n clusters as stated in the research paper\n",
    "        km_f_avg = KMeans(n_clusters= NO_OF_CLUSTER, random_state=0).fit(q_vec)\n",
    "    \n",
    "        cluster_label =km_f_avg.labels_\n",
    "        #print(collections.Counter(cluster_label))\n",
    "    \n",
    "    \n",
    "        new_q = np.empty([len(qids), 8*NO_OF_CLUSTER])\n",
    "        cluster_models = [0]*NO_OF_CLUSTER\n",
    "\n",
    "\n",
    "        for k in range(NO_OF_CLUSTER):\n",
    "            l = np.where(cluster_label == k)[0]\n",
    "            qids_cluster = [qids[idx] for idx in l]\n",
    "\n",
    "            # retrieve qids from indexes of the cluster\n",
    "            qids_cluster = [qids[idx] for idx in l]\n",
    "\n",
    "            TX_cluster = np.empty((0,np.size(TX, 1)), float)\n",
    "            Tqids_cluster = np.array([])\n",
    "            Ty_cluster = np.array([])\n",
    "\n",
    "            for qs in qids_cluster:\n",
    "                idxs = np.where(Tqids == qs)[0] # retrieve indexes of corresponding qid\n",
    "\n",
    "                # create data set that only contains the qids from the cluster\n",
    "                for i in idxs:\n",
    "                    TX_cluster = np.append(TX_cluster, [TX[i]], axis = 0)\n",
    "                    Tqids_cluster = np.append(Tqids_cluster, [Tqids[i]])\n",
    "                    Ty_cluster = np.append(Ty_cluster, [Ty[i]])\n",
    "\n",
    "\n",
    "\n",
    "            # train cluster\n",
    "            metric = pyltr.metrics.NDCG(k=5)\n",
    "            model = pyltr.models.LambdaMART(\n",
    "                metric=metric,\n",
    "                n_estimators=500,\n",
    "                verbose=0,\n",
    "            )\n",
    "            model.fit(TX_cluster, Ty_cluster, Tqids_cluster)\n",
    "\n",
    "            # store the fitted model\n",
    "            cluster_models[k] = model\n",
    "\n",
    "\n",
    "            # metrics from pyltr\n",
    "            metric1 = pyltr.metrics.NDCG(k=3)\n",
    "            metric2 = pyltr.metrics.NDCG(k=5)\n",
    "            metric3 = pyltr.metrics.NDCG(k=10)\n",
    "            metric4 = pyltr.metrics.AP(k=100)\n",
    "\n",
    "            for i, qid in enumerate(qids):\n",
    "                idxs = np.where(Tqids == qid)[0] # retrieve indexes of corresponding qid\n",
    "                TX_i = [TX[idx] for idx in idxs] # data\n",
    "                Ty_i = [Ty[idx] for idx in idxs] #l abels\n",
    "                rel_i = [Ty[idx] > 0 for idx in idxs]# boolean(relevant)\n",
    "\n",
    "                #prediction using the trained model\n",
    "                Ty_i = np.asarray(Ty_i)\n",
    "                Tpred_i = np.asarray(model.predict(TX_i))\n",
    "\n",
    "                # store metrics\n",
    "                first_idx = 8*k\n",
    "\n",
    "                new_q[i][first_idx] = metric1.evaluate_preds(i, Ty_i, Tpred_i) # ndcg@3\n",
    "                new_q[i][first_idx + 1] = metric2.evaluate_preds(i, Ty_i, Tpred_i) # ndcg@5\n",
    "                new_q[i][first_idx + 2] = metric3.evaluate_preds(i, Ty_i, Tpred_i) # ndcg@10\n",
    "                new_q[i][first_idx + 3] = metric4.evaluate_preds(i, Ty_i, Tpred_i) # MAP@100\n",
    "                new_q[i][first_idx + 4] = compute_MRR(rel_i, Tpred_i) # MRR@100\n",
    "                new_q[i][first_idx + 5] = compute_precision(rel_i, Tpred_i, 3) # p@3\n",
    "                new_q[i][first_idx + 6] = compute_precision(rel_i, Tpred_i, 5) # p@5\n",
    "                new_q[i][first_idx + 7] = compute_precision(rel_i, Tpred_i, 10)# p@10\n",
    "            #print('cluster ' + str(k) + ' done')\n",
    "        q_vec = new_q\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### Testing\n",
    "    \n",
    "    # from BFC, we know the strongest feature is index 38 for every fold\n",
    "    best_feature = 38\n",
    "    \n",
    "    # get the qids in the trainind data\n",
    "    e_qids = get_qids(Eqids)\n",
    "    t_qids = get_qids(Tqids)\n",
    "    \n",
    "    \n",
    "    ## Get top 10 values\n",
    "    \n",
    "    TX_top10 = np.empty((0,np.size(TX, 1)), float)\n",
    "    EX_top10 = np.empty((0,np.size(TX, 1)), float)\n",
    "\n",
    "    # iterate over qid for training\n",
    "    for i in t_qids: \n",
    "        idxs = np.where(Tqids == i)[0] # retrieve indexes of corresponding qid\n",
    "        TX_feature = np.array([TX[idx].tolist()for idx in idxs]) # list of best feature for all docs\n",
    "        sorted_array = TX_feature[TX_feature[:, best_feature].argsort()]\n",
    "        top10 = sorted_array[-10:]\n",
    "\n",
    "        TX_top10 = np.append(TX_top10, [np.mean(top10, axis = 0)], axis = 0)\n",
    "\n",
    "    # iterate over qid for evaluation\n",
    "    for i in e_qids: \n",
    "        idxs = np.where(Eqids == i)[0] # retrieve indexes of corresponding qid\n",
    "        EX_feature = np.array([EX[idx].tolist()for idx in idxs]) # list of best feature for all docs\n",
    "        sorted_array = EX_feature[EX_feature[:, best_feature].argsort()]\n",
    "        top10 = sorted_array[-10:]\n",
    "\n",
    "        EX_top10 = np.append(EX_top10, [np.mean(top10, axis = 0).tolist()], axis = 0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### logistics regression \n",
    "    clf = LogisticRegression(random_state=0).fit(TX_top10, cluster_label)\n",
    "    \n",
    "    #cluster model label of evaluation data set for selective cluster\n",
    "    label_SC = clf.predict(EX_top10)\n",
    "    prob = clf.predict_proba(EX_top10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # evaluation of selective cluster\n",
    "    ndcg_sc = evaluate_BFC_NDCG_selective_cluster(EX, Ey, Eqids, label_SC, cluster_models)\n",
    "    mrr_sc = evaluate_BFC_MRR_selective_cluster(EX, Ey, Eqids, label_SC, cluster_models)\n",
    "    \n",
    "    # evaluation of cluster oracle\n",
    "    ndcg_co = evaluate_BFC_NDCG_cluster_oracle(EX, Ey, Eqids, NO_OF_CLUSTER, cluster_models)\n",
    "    mrr_co = evaluate_BFC_MRR_cluster_oracle(EX, Ey, Eqids, NO_OF_CLUSTER, cluster_models)\n",
    "    \n",
    "    # evaluation of cluster fusion\n",
    "    ndcg_cf = evaluate_BFC_NDCG_cluster_fusion(EX, Ey, Eqids, prob, NO_OF_CLUSTER, cluster_models)\n",
    "    mrr_cf = evaluate_BFC_MRR_cluster_fusion(EX, Ey, Eqids, prob, NO_OF_CLUSTER, cluster_models)\n",
    "    \n",
    "    \n",
    "    # finally, add the evaluations\n",
    "    \n",
    "    Qid = np.append(Qid, Qids_e)\n",
    "    e_ndcg_sc = np.append(e_ndcg_sc, ndcg_sc)\n",
    "    e_mrr_sc = np.append(e_mrr_sc, mrr_sc)\n",
    "    e_ndcg_co = np.append(e_ndcg_co, ndcg_co)\n",
    "    e_mrr_co = np.append(e_mrr_co, mrr_co)\n",
    "    e_ndcg_cf = np.append(e_ndcg_cf, ndcg_cf)\n",
    "    e_mrr_cf = np.append(e_mrr_cf, mrr_cf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qid_df = pd.DataFrame.from_dict(Qid) \n",
    "e_ndcg_sc_df = pd.DataFrame.from_dict(e_ndcg_sc) \n",
    "e_mrr_sc_df = pd.DataFrame.from_dict(e_mrr_sc) \n",
    "e_ndcg_co_df = pd.DataFrame.from_dict(e_ndcg_co) \n",
    "e_mrr_co_df = pd.DataFrame.from_dict(e_mrr_co) \n",
    "e_ndcg_cf_df = pd.DataFrame.from_dict(e_ndcg_cf) \n",
    "e_mrr_cf_df = pd.DataFrame.from_dict(e_mrr_cf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qid_df.columns=['Q_ID']\n",
    "e_ndcg_sc_df.columns=['NDCG@5']\n",
    "e_ndcg_co_df.columns=['NDCG@5']\n",
    "e_mrr_sc_df.columns=['MRR@100']\n",
    "e_mrr_co_df.columns=['MRR@100']\n",
    "e_ndcg_cf_df.columns=['NDCG@5']\n",
    "e_mrr_cf_df.columns=['MRR@100']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qid_df.to_csv('Qid_kmeans.csv', index = False)\n",
    "\n",
    "e_ndcg_sc_df.to_csv('ndcg_sc_kmeans.csv', index = False)\n",
    "e_ndcg_co_df.to_csv('ndcg_co_kmeans.csv', index = False)\n",
    "e_mrr_sc_df.to_csv('mrr_sc_kmeans.csv', index = False)\n",
    "e_mrr_co_df.to_csv('mrr_co_kmeans.csv', index = False)\n",
    "e_ndcg_cf_df.to_csv('ndcg_cf_kmeans.csv', index = False)\n",
    "e_mrr_cf_df.to_csv('mrr_cf_kmeans.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7968., 7979., 7993., ..., 7943., 7953., 7959.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
