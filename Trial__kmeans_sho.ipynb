{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyltr in /opt/conda/lib/python3.7/site-packages (0.2.4)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from pyltr) (1.0.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from pyltr) (1.18.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from pyltr) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from pyltr) (0.22.2.post1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas->pyltr) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->pyltr) (2019.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->pyltr) (0.14.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas->pyltr) (1.14.0)\n",
      "Requirement already satisfied: more_itertools in /opt/conda/lib/python3.7/site-packages (8.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyltr\n",
    "!pip install more_itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pyltr\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "from more_itertools import sort_together\n",
    "import collections\n",
    "import heapq\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input array of qids of each query-document pair\n",
    "# output sorted qids \n",
    "def get_qids(Qids):\n",
    "    qs = list(set(Qids))\n",
    "    qs.sort()\n",
    "    \n",
    "    return qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./MQ2007/Fold1/train.txt') as trainfile, \\\n",
    "         open('./MQ2007/Fold1/vali.txt') as valifile, \\\n",
    "         open('./MQ2007/Fold1/test.txt') as evalfile:\n",
    "    TX, Ty, Tqids, _ = pyltr.data.letor.read_dataset(trainfile)\n",
    "    VX, Vy, Vqids, _ = pyltr.data.letor.read_dataset(valifile)\n",
    "    EX, Ey, Eqids, _ = pyltr.data.letor.read_dataset(evalfile)\n",
    "\n",
    "Tqids = np.array([int(i) for i in Tqids])\n",
    "Vqids = np.array([int(i) for i in Vqids])\n",
    "Eqids = np.array([int(i) for i in Eqids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the qids in the trainind data\n",
    "qids = get_qids(Tqids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average of features of relevant documents for query IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_features_qids = np.empty((0,np.size(TX, 1)), float)\n",
    "\n",
    "# iterate over qid\n",
    "for i in qids: \n",
    "    idxs = np.where(Tqids == i)[0] # retrieve indexes of corresponding qid\n",
    "    relevance_list = [Ty[idx] > 0 for idx in idxs] # list of boolean(relevant) of q-d pairs\n",
    "    rel_idxs =np.where(relevance_list)[0] # retrieve indexes of relevant docs\n",
    "    \n",
    "    ### if np where is never empty\n",
    "    rel_doc_list = [TX[idx] for idx in rel_idxs] # retrieve relevant docs\n",
    "    avg_features = np.mean(rel_doc_list, axis=0) #average each feature of relevant docs\n",
    "    \n",
    "    if any(rel_idxs):\n",
    "        rel_doc_list = [TX[idx] for idx in rel_idxs]\n",
    "        avg_features = np.mean(rel_doc_list, axis=0) #average each feature of relevant docs\n",
    "    else:\n",
    "        avg_features = np.zeros(np.size(TX, 1)) # if there is no relevant document in training, give features 0s\n",
    "    \n",
    "    avg_features_qids = np.append(avg_features_qids, [avg_features], axis = 0)\n",
    "\n",
    "q_vec = avg_features_qids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision(relevance_list, feature_list, k):\n",
    "    feature_list, relevance_list = (list(t) for t in zip(*sorted(zip(feature_list, relevance_list), reverse = True)))\n",
    "    if len(feature_list) > k:\n",
    "        feature_list = feature_list[0:k]\n",
    "        relevance_list = relevance_list[0:k]\n",
    "    else:\n",
    "        k = len(feature_list)\n",
    "    \n",
    "    return np.count_nonzero(relevance_list)/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_MRR(relevance_list, feature_list):\n",
    "    # sort arrays by the feature value\n",
    "    feature_list, relevance_list = (list(t) for t in zip(*sorted(zip(feature_list, relevance_list), reverse = True)))\n",
    "    # MRR@100, only look at top 100\n",
    "    if len(feature_list) > 100:\n",
    "        feature_list = feature_list[0:100]\n",
    "        relevance_list = relevance_list[0:100]\n",
    "\n",
    "    idx = np.where(relevance_list)[0]#get the indexesof relevant document\n",
    "    if any(idx):\n",
    "        return 1/(idx[0]+1) # index starts from 0 so add 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster once with average features of relevant docs, and twice more using evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 521, 3: 173, 0: 156, 4: 144, 2: 23})\n",
      "cluster 0 done\n",
      "cluster 1 done\n",
      "cluster 2 done\n",
      "cluster 3 done\n",
      "cluster 4 done\n",
      "Counter({1: 268, 0: 209, 4: 195, 3: 186, 2: 159})\n",
      "cluster 0 done\n",
      "cluster 1 done\n",
      "cluster 2 done\n",
      "cluster 3 done\n",
      "cluster 4 done\n",
      "Counter({0: 298, 1: 203, 4: 183, 3: 178, 2: 155})\n",
      "cluster 0 done\n",
      "cluster 1 done\n",
      "cluster 2 done\n",
      "cluster 3 done\n",
      "cluster 4 done\n"
     ]
    }
   ],
   "source": [
    "NO_OF_CLUSTER=5\n",
    "\n",
    "for count in range(3):\n",
    "    ##cluster the feature into n clusters as stated in the research paper\n",
    "    km_f_avg = KMeans(n_clusters= NO_OF_CLUSTER, random_state=0).fit(q_vec)\n",
    "    \n",
    "    cluster_label =km_f_avg.labels_\n",
    "    print(collections.Counter(cluster_label))\n",
    "    \n",
    "    \n",
    "    new_q = np.empty([len(qids), 8*NO_OF_CLUSTER])\n",
    "    cluster_models = [0]*NO_OF_CLUSTER\n",
    "    \n",
    "    \n",
    "    for k in range(NO_OF_CLUSTER):\n",
    "        l = np.where(cluster_label == k)[0]\n",
    "        qids_cluster = [qids[idx] for idx in l]\n",
    "    \n",
    "        # retrieve qids from indexes of the cluster\n",
    "        qids_cluster = [qids[idx] for idx in l]\n",
    "\n",
    "        TX_cluster = np.empty((0,np.size(TX, 1)), float)\n",
    "        Tqids_cluster = np.array([])\n",
    "        Ty_cluster = np.array([])\n",
    "\n",
    "        for qs in qids_cluster:\n",
    "            idxs = np.where(Tqids == qs)[0] # retrieve indexes of corresponding qid\n",
    "    \n",
    "            # create data set that only contains the qids from the cluster\n",
    "            for i in idxs:\n",
    "                TX_cluster = np.append(TX_cluster, [TX[i]], axis = 0)\n",
    "                Tqids_cluster = np.append(Tqids_cluster, [Tqids[i]])\n",
    "                Ty_cluster = np.append(Ty_cluster, [Ty[i]])\n",
    "    \n",
    "    \n",
    "    \n",
    "        # train cluster\n",
    "        metric = pyltr.metrics.NDCG(k=5)\n",
    "        model = pyltr.models.LambdaMART(\n",
    "            metric=metric,\n",
    "            n_estimators=50,\n",
    "            verbose=0,\n",
    "        )\n",
    "        model.fit(TX_cluster, Ty_cluster, Tqids_cluster)\n",
    "    \n",
    "        # store the fitted model\n",
    "        cluster_models[k] = model\n",
    "\n",
    "    \n",
    "        # metrics from pyltr\n",
    "        metric1 = pyltr.metrics.NDCG(k=3)\n",
    "        metric2 = pyltr.metrics.NDCG(k=5)\n",
    "        metric3 = pyltr.metrics.NDCG(k=10)\n",
    "        metric4 = pyltr.metrics.AP(k=100)\n",
    "\n",
    "        for i, qid in enumerate(qids):\n",
    "            idxs = np.where(Tqids == qid)[0] # retrieve indexes of corresponding qid\n",
    "            TX_i = [TX[idx] for idx in idxs] # data\n",
    "            Ty_i = [Ty[idx] for idx in idxs] #l abels\n",
    "            rel_i = [Ty[idx] > 0 for idx in idxs]# boolean(relevant)\n",
    "    \n",
    "            Tpred_i = model.predict(TX_i)\n",
    "            Ty_i = np.asarray(Ty_i)\n",
    "            Tpred_i = np.asarray(Tpred_i)\n",
    "    \n",
    "            # store metrics\n",
    "            first_idx = 8*k\n",
    "\n",
    "            new_q[i][first_idx] = metric1.evaluate_preds(i, Ty_i, Tpred_i) # ndcg@3\n",
    "            new_q[i][first_idx + 1] = metric2.evaluate_preds(i, Ty_i, Tpred_i) # ndcg@5\n",
    "            new_q[i][first_idx + 2] = metric3.evaluate_preds(i, Ty_i, Tpred_i) # ndcg@10\n",
    "            new_q[i][first_idx + 3] = metric4.evaluate_preds(i, Ty_i, Tpred_i) # MAP@100\n",
    "            new_q[i][first_idx + 4] = compute_MRR(rel_i, Tpred_i) # MRR@100\n",
    "            new_q[i][first_idx + 5] = compute_precision(rel_i, Tpred_i, 3) # p@3\n",
    "            new_q[i][first_idx + 6] = compute_precision(rel_i, Tpred_i, 5) # p@5\n",
    "            new_q[i][first_idx + 7] = compute_precision(rel_i, Tpred_i, 10)# p@10\n",
    "        print('cluster ' + str(k) + ' done')\n",
    "    q_vec = new_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyltr.models.lambdamart.LambdaMART at 0x7f7401818f10>,\n",
       " <pyltr.models.lambdamart.LambdaMART at 0x7f740182d050>,\n",
       " <pyltr.models.lambdamart.LambdaMART at 0x7f740182d110>,\n",
       " <pyltr.models.lambdamart.LambdaMART at 0x7f740182ddd0>,\n",
       " <pyltr.models.lambdamart.LambdaMART at 0x7f7401826050>]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#after clustering 3 times, cluster_models give the models trained with the clusters for the 3rd iteration\n",
    "cluster_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## choosing clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from BFC, we know the strongest feature is index 38 for every fold\n",
    "best_feature = 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the qids in the trainind data\n",
    "e_qids = get_qids(Eqids)\n",
    "t_qids = get_qids(Tqids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get top 10 values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "TX_top10 = np.empty((0,np.size(TX, 1)), float)\n",
    "EX_top10 = np.empty((0,np.size(TX, 1)), float)\n",
    "\n",
    "# iterate over qid for training\n",
    "for i in t_qids: \n",
    "    idxs = np.where(Tqids == i)[0] # retrieve indexes of corresponding qid\n",
    "    TX_feature = np.array([TX[idx].tolist()for idx in idxs]) # list of best feature for all docs\n",
    "    sorted_array = TX_feature[TX_feature[:, best_feature].argsort()]\n",
    "    top10 = sorted_array[-10:]\n",
    "    \n",
    "    TX_top10 = np.append(TX_top10, [np.mean(top10, axis = 0)], axis = 0)\n",
    "\n",
    "# iterate over qid for evaluation\n",
    "for i in e_qids: \n",
    "    idxs = np.where(Eqids == i)[0] # retrieve indexes of corresponding qid\n",
    "    EX_feature = np.array([EX[idx].tolist()for idx in idxs]) # list of best feature for all docs\n",
    "    sorted_array = EX_feature[EX_feature[:, best_feature].argsort()]\n",
    "    top10 = sorted_array[-10:]\n",
    "    \n",
    "    EX_top10 = np.append(EX_top10, [np.mean(top10, axis = 0).tolist()], axis = 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(EX_top10[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(TX_top10, cluster_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster model label of evaluation data set for selective cluster\n",
    "label_SC = clf.predict(EX_top10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
