{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyltr in /opt/conda/lib/python3.7/site-packages (0.2.4)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from pyltr) (0.22.2.post1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from pyltr) (1.0.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from pyltr) (1.18.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from pyltr) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->pyltr) (0.14.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->pyltr) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas->pyltr) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas->pyltr) (1.14.0)\n",
      "Requirement already satisfied: more_itertools in /opt/conda/lib/python3.7/site-packages (8.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyltr\n",
    "!pip install more_itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import pyltr\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "from more_itertools import sort_together\n",
    "import collections\n",
    "import heapq\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input array of qids of each query-document pair\n",
    "# output sorted qids \n",
    "def get_qids(Qids):\n",
    "    qs = list(set(Qids))\n",
    "    qs.sort()\n",
    "    \n",
    "    return qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input evaluation data and cluster number\n",
    "## output an array of ndcg score for each qids\n",
    "def evaluate_BFC_NDCG_selective_cluster(EX, Ey, Eqids, E_k, models):\n",
    "    \n",
    "    # get a list of qids\n",
    "    eqs = get_qids(Eqids)\n",
    "    metric = pyltr.metrics.NDCG(k=5)\n",
    "    ndcg_qs = np.array([])\n",
    "    \n",
    "    for i, eq in enumerate(eqs):\n",
    "        idxs = np.where(Eqids == eq)[0]\n",
    "        EX_i = [EX[idx] for idx in idxs] # target score\n",
    "        relevance_list = [Ey[idx] for idx in idxs] # true score\n",
    "        \n",
    "        # select model from predicted cluster\n",
    "        model = models[E_k[i]]\n",
    "        Epred_i = model.predict(EX_i)\n",
    "        \n",
    "        score = metric.evaluate_preds(i, np.asarray(relevance_list), np.asarray(Epred_i))\n",
    "        ndcg_qs = np.append(ndcg_qs, [score], axis = 0)\n",
    "        \n",
    "    return ndcg_qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input evaluation data and cluster number\n",
    "## output an array of ndcg score for each qids\n",
    "def evaluate_BFC_NDCG_cluster_oracle(EX, Ey, Eqids, no_cluster, models):\n",
    "    \n",
    "    # get a list of qids\n",
    "    eqs = get_qids(Eqids)\n",
    "    metric = pyltr.metrics.NDCG(k=5)\n",
    "    ndcg_qs = np.array([])\n",
    "    \n",
    "    for i, eq in enumerate(eqs):\n",
    "        idxs = np.where(Eqids == eq)[0]\n",
    "        EX_i = [EX[idx] for idx in idxs] # target score\n",
    "        relevance_list = [Ey[idx] for idx in idxs] # true score\n",
    "        \n",
    "        best = 0\n",
    "        # choose highest performance of 5 models\n",
    "        for k in range(no_cluster):\n",
    "            model = models[k]\n",
    "            Epred_i = model.predict(EX_i)\n",
    "            score = metric.evaluate_preds(i, np.asarray(relevance_list), np.asarray(Epred_i))\n",
    "            if score > best:\n",
    "                best = score\n",
    "        \n",
    "\n",
    "        ndcg_qs = np.append(ndcg_qs, [best], axis = 0)\n",
    "        \n",
    "    return ndcg_qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input evaluation data and cluster number\n",
    "## output an array of MRR score for each qids\n",
    "def evaluate_BFC_MRR_selective_cluster(EX, Ey, Eqids, E_k, models):\n",
    "    \n",
    "    # get a list of qids\n",
    "    eqs = get_qids(Eqids)\n",
    "    mrr_qs = np.array([])\n",
    "    \n",
    "    \n",
    "    for i, eq in enumerate(eqs):\n",
    "        idxs = np.where(Eqids == eq)[0]\n",
    "        EX_i = [EX[idx] for idx in idxs] # target score\n",
    "        relevance_list = [Ey[idx] > 0 for idx in idxs] # true score, convert a boolean list (relevant or non relevant)\n",
    "        \n",
    "        # select model from predicted cluster\n",
    "        model = models[E_k[i]]\n",
    "        Epred_i = model.predict(EX_i)\n",
    "        \n",
    "        # get the evaluation score for the query\n",
    "        score = compute_MRR(relevance_list, np.asarray(Epred_i))\n",
    "        mrr_qs = np.append(mrr_qs, [score], axis = 0)\n",
    "\n",
    "    return mrr_qs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input evaluation data and cluster number\n",
    "## output an array of MRR score for each qids\n",
    "def evaluate_BFC_MRR_cluster_oracle(EX, Ey, Eqids, no_cluster, models):\n",
    "    \n",
    "    # get a list of qids\n",
    "    eqs = get_qids(Eqids)\n",
    "    mrr_qs = np.array([])\n",
    "    \n",
    "    \n",
    "    for i, eq in enumerate(eqs):\n",
    "        idxs = np.where(Eqids == eq)[0]\n",
    "        EX_i = [EX[idx] for idx in idxs] # target score\n",
    "        relevance_list = [Ey[idx] > 0 for idx in idxs] # true score, convert a boolean list (relevant or non relevant)\n",
    "        \n",
    "        best = 0\n",
    "        # choose highest performance of 5 models\n",
    "        for k in range(no_cluster):\n",
    "            model = models[k]\n",
    "            Epred_i = model.predict(EX_i)\n",
    "            score = compute_MRR(relevance_list, np.asarray(Epred_i))\n",
    "            if score > best:\n",
    "                best = score\n",
    "\n",
    "        mrr_qs = np.append(mrr_qs, [best], axis = 0)\n",
    "\n",
    "    return mrr_qs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./MQ2007/Fold1/train.txt') as trainfile, \\\n",
    "         open('./MQ2007/Fold1/vali.txt') as valifile, \\\n",
    "         open('./MQ2007/Fold1/test.txt') as evalfile:\n",
    "    TX, Ty, Tqids, _ = pyltr.data.letor.read_dataset(trainfile)\n",
    "    VX, Vy, Vqids, _ = pyltr.data.letor.read_dataset(valifile)\n",
    "    EX, Ey, Eqids, _ = pyltr.data.letor.read_dataset(evalfile)\n",
    "\n",
    "Tqids = np.array([int(i) for i in Tqids])\n",
    "Vqids = np.array([int(i) for i in Vqids])\n",
    "Eqids = np.array([int(i) for i in Eqids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the qids in the trainind data\n",
    "qids = get_qids(Tqids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average of features of relevant documents for query IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_features_qids = np.empty((0,np.size(TX, 1)), float)\n",
    "\n",
    "# iterate over qid\n",
    "for i in qids: \n",
    "    idxs = np.where(Tqids == i)[0] # retrieve indexes of corresponding qid\n",
    "    relevance_list = [Ty[idx] > 0 for idx in idxs] # list of boolean(relevant) of q-d pairs\n",
    "    rel_idxs =np.where(relevance_list)[0] # retrieve indexes of relevant docs\n",
    "    \n",
    "    ### if np where is never empty\n",
    "    rel_doc_list = [TX[idx] for idx in rel_idxs] # retrieve relevant docs\n",
    "    avg_features = np.mean(rel_doc_list, axis=0) #average each feature of relevant docs\n",
    "    \n",
    "    if any(rel_idxs):\n",
    "        rel_doc_list = [TX[idx] for idx in rel_idxs]\n",
    "        avg_features = np.mean(rel_doc_list, axis=0) #average each feature of relevant docs\n",
    "    else:\n",
    "        avg_features = np.zeros(np.size(TX, 1)) # if there is no relevant document in training, give features 0s\n",
    "    \n",
    "    avg_features_qids = np.append(avg_features_qids, [avg_features], axis = 0)\n",
    "\n",
    "q_vec = avg_features_qids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision(relevance_list, feature_list, k):\n",
    "    feature_list, relevance_list = (list(t) for t in zip(*sorted(zip(feature_list, relevance_list), reverse = True)))\n",
    "    if len(feature_list) > k:\n",
    "        feature_list = feature_list[0:k]\n",
    "        relevance_list = relevance_list[0:k]\n",
    "    else:\n",
    "        k = len(feature_list)\n",
    "    \n",
    "    return np.count_nonzero(relevance_list)/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_MRR(relevance_list, feature_list):\n",
    "    # sort arrays by the feature value\n",
    "    feature_list, relevance_list = (list(t) for t in zip(*sorted(zip(feature_list, relevance_list), reverse = True)))\n",
    "    # MRR@100, only look at top 100\n",
    "    if len(feature_list) > 100:\n",
    "        feature_list = feature_list[0:100]\n",
    "        relevance_list = relevance_list[0:100]\n",
    "\n",
    "    idx = np.where(relevance_list)[0]#get the indexesof relevant document\n",
    "    if any(idx):\n",
    "        return 1/(idx[0]+1) # index starts from 0 so add 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster once with average features of relevant docs, and twice more using evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 521, 3: 173, 0: 156, 4: 144, 2: 23})\n",
      "cluster 0 done\n",
      "cluster 1 done\n",
      "cluster 2 done\n",
      "cluster 3 done\n",
      "cluster 4 done\n",
      "Counter({1: 292, 2: 240, 3: 239, 0: 210, 4: 36})\n",
      "cluster 0 done\n",
      "cluster 1 done\n",
      "cluster 2 done\n",
      "cluster 3 done\n",
      "cluster 4 done\n",
      "Counter({3: 259, 1: 251, 0: 234, 2: 179, 4: 94})\n",
      "cluster 0 done\n",
      "cluster 1 done\n",
      "cluster 2 done\n",
      "cluster 3 done\n",
      "cluster 4 done\n"
     ]
    }
   ],
   "source": [
    "NO_OF_CLUSTER=5\n",
    "\n",
    "for count in range(3):\n",
    "    ##cluster the feature into n clusters as stated in the research paper\n",
    "    km_f_avg = KMeans(n_clusters= NO_OF_CLUSTER, random_state=0).fit(q_vec)\n",
    "    \n",
    "    cluster_label =km_f_avg.labels_\n",
    "    print(collections.Counter(cluster_label))\n",
    "    \n",
    "    \n",
    "    new_q = np.empty([len(qids), 8*NO_OF_CLUSTER])\n",
    "    cluster_models = [0]*NO_OF_CLUSTER\n",
    "    \n",
    "    \n",
    "    for k in range(NO_OF_CLUSTER):\n",
    "        l = np.where(cluster_label == k)[0]\n",
    "        qids_cluster = [qids[idx] for idx in l]\n",
    "    \n",
    "        # retrieve qids from indexes of the cluster\n",
    "        qids_cluster = [qids[idx] for idx in l]\n",
    "\n",
    "        TX_cluster = np.empty((0,np.size(TX, 1)), float)\n",
    "        Tqids_cluster = np.array([])\n",
    "        Ty_cluster = np.array([])\n",
    "\n",
    "        for qs in qids_cluster:\n",
    "            idxs = np.where(Tqids == qs)[0] # retrieve indexes of corresponding qid\n",
    "    \n",
    "            # create data set that only contains the qids from the cluster\n",
    "            for i in idxs:\n",
    "                TX_cluster = np.append(TX_cluster, [TX[i]], axis = 0)\n",
    "                Tqids_cluster = np.append(Tqids_cluster, [Tqids[i]])\n",
    "                Ty_cluster = np.append(Ty_cluster, [Ty[i]])\n",
    "    \n",
    "    \n",
    "    \n",
    "        # train cluster\n",
    "        metric = pyltr.metrics.NDCG(k=5)\n",
    "        model = pyltr.models.LambdaMART(\n",
    "            metric=metric,\n",
    "            n_estimators=5,\n",
    "            verbose=0,\n",
    "        )\n",
    "        model.fit(TX_cluster, Ty_cluster, Tqids_cluster)\n",
    "    \n",
    "        # store the fitted model\n",
    "        cluster_models[k] = model\n",
    "\n",
    "    \n",
    "        # metrics from pyltr\n",
    "        metric1 = pyltr.metrics.NDCG(k=3)\n",
    "        metric2 = pyltr.metrics.NDCG(k=5)\n",
    "        metric3 = pyltr.metrics.NDCG(k=10)\n",
    "        metric4 = pyltr.metrics.AP(k=100)\n",
    "\n",
    "        for i, qid in enumerate(qids):\n",
    "            idxs = np.where(Tqids == qid)[0] # retrieve indexes of corresponding qid\n",
    "            TX_i = [TX[idx] for idx in idxs] # data\n",
    "            Ty_i = [Ty[idx] for idx in idxs] #l abels\n",
    "            rel_i = [Ty[idx] > 0 for idx in idxs]# boolean(relevant)\n",
    "    \n",
    "            #prediction using the trained model\n",
    "            Ty_i = np.asarray(Ty_i)\n",
    "            Tpred_i = np.asarray(model.predict(TX_i))\n",
    "    \n",
    "            # store metrics\n",
    "            first_idx = 8*k\n",
    "\n",
    "            new_q[i][first_idx] = metric1.evaluate_preds(i, Ty_i, Tpred_i) # ndcg@3\n",
    "            new_q[i][first_idx + 1] = metric2.evaluate_preds(i, Ty_i, Tpred_i) # ndcg@5\n",
    "            new_q[i][first_idx + 2] = metric3.evaluate_preds(i, Ty_i, Tpred_i) # ndcg@10\n",
    "            new_q[i][first_idx + 3] = metric4.evaluate_preds(i, Ty_i, Tpred_i) # MAP@100\n",
    "            new_q[i][first_idx + 4] = compute_MRR(rel_i, Tpred_i) # MRR@100\n",
    "            new_q[i][first_idx + 5] = compute_precision(rel_i, Tpred_i, 3) # p@3\n",
    "            new_q[i][first_idx + 6] = compute_precision(rel_i, Tpred_i, 5) # p@5\n",
    "            new_q[i][first_idx + 7] = compute_precision(rel_i, Tpred_i, 10)# p@10\n",
    "        print('cluster ' + str(k) + ' done')\n",
    "    q_vec = new_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyltr.models.lambdamart.LambdaMART at 0x7fe845684b10>,\n",
       " <pyltr.models.lambdamart.LambdaMART at 0x7fe844e7d850>,\n",
       " <pyltr.models.lambdamart.LambdaMART at 0x7fe844e7df90>,\n",
       " <pyltr.models.lambdamart.LambdaMART at 0x7fe844e7da10>,\n",
       " <pyltr.models.lambdamart.LambdaMART at 0x7fe844e7d750>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#after clustering 3 times, cluster_models give the models trained with the clusters for the 3rd iteration\n",
    "cluster_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## choosing clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from BFC, we know the strongest feature is index 38 for every fold\n",
    "best_feature = 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the qids in the trainind data\n",
    "e_qids = get_qids(Eqids)\n",
    "t_qids = get_qids(Tqids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get top 10 values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TX_top10 = np.empty((0,np.size(TX, 1)), float)\n",
    "EX_top10 = np.empty((0,np.size(TX, 1)), float)\n",
    "\n",
    "# iterate over qid for training\n",
    "for i in t_qids: \n",
    "    idxs = np.where(Tqids == i)[0] # retrieve indexes of corresponding qid\n",
    "    TX_feature = np.array([TX[idx].tolist()for idx in idxs]) # list of best feature for all docs\n",
    "    sorted_array = TX_feature[TX_feature[:, best_feature].argsort()]\n",
    "    top10 = sorted_array[-10:]\n",
    "    \n",
    "    TX_top10 = np.append(TX_top10, [np.mean(top10, axis = 0)], axis = 0)\n",
    "\n",
    "# iterate over qid for evaluation\n",
    "for i in e_qids: \n",
    "    idxs = np.where(Eqids == i)[0] # retrieve indexes of corresponding qid\n",
    "    EX_feature = np.array([EX[idx].tolist()for idx in idxs]) # list of best feature for all docs\n",
    "    sorted_array = EX_feature[EX_feature[:, best_feature].argsort()]\n",
    "    top10 = sorted_array[-10:]\n",
    "    \n",
    "    EX_top10 = np.append(EX_top10, [np.mean(top10, axis = 0).tolist()], axis = 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(TX_top10, cluster_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster model label of evaluation data set for selective cluster\n",
    "label_SC = clf.predict(EX_top10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation of selective cluster\n",
    "ndcg_sc = evaluate_BFC_NDCG_selective_cluster(EX, Ey, Eqids, label_SC, cluster_models)\n",
    "mrr_sc = evaluate_BFC_MRR_selective_cluster(EX, Ey, Eqids, label_SC, cluster_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation of cluster oracle\n",
    "ndcg_co = evaluate_BFC_NDCG_cluster_oracle(EX, Ey, Eqids, NO_OF_CLUSTER, cluster_models)\n",
    "mrr_co = evaluate_BFC_MRR_cluster_oracle(EX, Ey, Eqids, NO_OF_CLUSTER, cluster_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5300739955500013"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(mrr_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
